diff --git a/train.py b/train.py
index 0ee4636..7d443d7 100644
--- a/train.py
+++ b/train.py
@@ -1,7 +1,7 @@
 import torch
 from torch.nn import functional as F
 import torch.nn as nn
-from accelerator import Accelerator
+from accelerate import Accelerator
 import lpips
 from diffusers.models import AutoencoderKL
 from diffusers import StableDiffusionPipeline
@@ -31,6 +31,8 @@ def parse_args():
                         help='Name of the dataset to use for training')
     parser.add_argument('--learning_rate', type=float, default=1e-4,
                         help='Learning rate')
+    parser.add_argument('--model_type', type=str, default='fp16',
+                        help="Model type: 'fp32' or 'fp16'")
 
     return parser.parse_args()
 
@@ -40,7 +42,9 @@ args = parse_args()
 device = "mps"
 
 def get_consistency_decoder():
+    print("Loading consistency decoder")
     decoder_consistency = ConsistencyDecoder(device=device) # Model size: 2.49 GB
+    print("Consistency decoder loaded")
     return decoder_consistency
 
 def get_sdxl_vae():
@@ -89,12 +93,12 @@ def load_image(uri, size=None, center_crop=False):
 # Encode with SD vae and decode with consistency decoder
 #
 # Accepts a PIL image
-def test_sample(image_file):
-    # pil to tensor
-    image_tensor = load_image(image_file).to(device=device)
-    latent_sd_15 = pipe.vae.encode(image_tensor.half().cuda()).latent_dist.mean
+# def test_sample(image_file):
+#     # pil to tensor
+#     image_tensor = load_image(image_file).to(device=device)
+#     latent_sd_15 = pipe.vae.encode(image_tensor.half().cuda()).latent_dist.mean
 
-    return image
+#     return image
 
 
 # Defining a simple MLP (Multi-Layer Perceptron) class named 'LatentProjector'.
@@ -115,31 +119,89 @@ class LatentProjector(nn.Module):
 
 
 # Loading dataset_name from args
+# def get_train_dataloader(args):
+#     dataset_name = args.dataset_name
+
+#     dataset = load_dataset(dataset_name)["train"]
+#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.train_batch_size, shuffle=True)
+
+#     return dataloader   
+
+from torchvision.transforms import ToTensor, Resize
+
+# def get_train_dataloader(args):
+#     dataset_name = args.dataset_name
+#     dataset = load_dataset(dataset_name)["train"]
+#     transform = ToTensor()
+#     print("Mapping")
+    
+#     # for testing
+#     dataset = dataset.select(range(100))
+#     dataset = dataset.map(lambda x: {"image": transform(x["image"])})
+#     print("Done mapping")
+#     dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.train_batch_size, shuffle=True)
+#     return dataloader
+
+
+import torchvision
+from torchvision.transforms import ToTensor, CenterCrop
+
 def get_train_dataloader(args):
     dataset_name = args.dataset_name
-
     dataset = load_dataset(dataset_name)["train"]
-    dataloader = torch.utils.data.DataLoader(dataset, batch_size=args.train_batch_size, shuffle=True)
-
-    return dataloader   
+    min_size = 1280  # Define your minimum size
+    transform = torchvision.transforms.Compose([
+        CenterCrop(min_size),  # Center crop all images to have the same size
+        ToTensor()  # Convert PIL Image to tensor
+    ])
+
+    # Use only the first 100 items for debugging
+    dataset = dataset.select(range(100))
+
+    print("Mapping")
+    all_items = []
+    for i, item in enumerate(dataset):
+        try:
+            print(f"Mapping item {i}")
+            # Check if the image size is larger than the minimum size
+            if item["image"].size[0] >= min_size and item["image"].size[1] >= min_size:
+                item = {"pixel_values": transform(item["image"])}
+                all_items.append(item)
+            else:
+                print(f"Skipping item {i} due to small size")
+        except Exception as e:
+            print(f"Error while mapping item {i}: {e}")
+
+    print("Done mapping")
+
+    dataloader = torch.utils.data.DataLoader(all_items, batch_size=args.train_batch_size, shuffle=True)
+    return dataloader
 
 
 def train():
     wandb.init()
-    
-    accelerator = Accelerator(fp16=True, device=device)
-    train_dataloader = get_train_dataloader()
 
-    lpips_loss_fn = lpips.LPIPS(net='alex').to(accelerator.device)
+    # accelerator = Accelerator(fp16=True, device=device)
+    train_dataloader = get_train_dataloader(args)
 
-    latent_projetor = LatentProjector().to(accelerator.device)
+    # map args.model_type to torch dtype
+    type_map ={
+        "fp32": torch.float32,
+        "fp16": torch.float16,
+    }
+    dtype = type_map[args.model_type]
+
+    lpips_loss_fn = lpips.LPIPS(net='alex').to(device, dtype=dtype)
+
+    latent_projetor = LatentProjector().to(device, dtype=dtype)
     latent_projetor.train()
 
     sdxl_vae = get_sd_vae()
     consitency_decoder = get_consistency_decoder()
 
+
     weight_dtype = torch.float16
-    logger = accelerator.get_logger()
+    # logger = accelerator.get_logger()
 
     params_to_optimize = latent_projetor.parameters()
 
@@ -149,37 +211,44 @@ def train():
     for epoch in range(0, args.num_train_epochs):
         train_loss = 0.0
         for step, batch in enumerate(train_dataloader):
-            with accelerator.accumulate(latent_projetor):
-                target = batch["pixel_values"].to(weight_dtype)
-
-                # https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoder_kl.py
-                posterior = sdxl_vae.encode(target).latent_dist
-                z = posterior.mode()
-                pred = consitency_decoder(z).sample
-
-                kl_loss = posterior.kl().mean()
-                mse_loss = F.mse_loss(pred, target, reduction="mean")
-                lpips_loss = lpips_loss_fn(pred, target).mean()
-
-                logger.info(f'mse:{mse_loss.item()}, lpips:{lpips_loss.item()}, kl:{kl_loss.item()}')
-
-                loss = mse_loss + args.lpips_scale * lpips_loss + args.kl_scale * kl_loss
-                
-
-                # log all losses
-                wandb.log({
-                    "loss": loss.item(),
-                    "mse_loss": mse_loss.item(),
-                    "lpips_loss": lpips_loss.item(),
-                    "kl_loss": kl_loss.item(),
-                })
-
-                # Gather the losses across all processes for logging (if we use distributed training).
-                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
-                train_loss += avg_loss.item() / args.gradient_accumulation_steps
-
-
-                accelerator.backward(loss)
-                optimizer.step()
-                lr_scheduler.step()
-                optimizer.zero_grad()
+            # with accelerator.accumulate(latent_projetor):
+            target = batch["pixel_values"].to(weight_dtype)
+
+            # https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoder_kl.py
+            posterior = sdxl_vae.encode(target).latent_dist
+            z = posterior.mode()
+            pred = consitency_decoder(z).sample
+
+            kl_loss = posterior.kl().mean()
+            mse_loss = F.mse_loss(pred, target, reduction="mean")
+            lpips_loss = lpips_loss_fn(pred, target).mean()
+
+            # logger.info(f'mse:{mse_loss.item()}, lpips:{lpips_loss.item()}, kl:{kl_loss.item()}')
+
+            loss = mse_loss + args.lpips_scale * lpips_loss + args.kl_scale * kl_loss
+            
+
+            # log all losses
+            wandb.log({
+                "loss": loss.item(),
+                "mse_loss": mse_loss.item(),
+                "lpips_loss": lpips_loss.item(),
+                "kl_loss": kl_loss.item(),
+            })
+
+            # Gather the losses across all processes for logging (if we use distributed training).
+            # avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
+            # without accelerator
+            avg_loss = loss.mean()
+            train_loss += avg_loss.item() / args.gradient_accumulation_steps
+
+
+            # accelerator.backward(loss)
+            # without accelerator
+            loss.backward()
+            optimizer.step()
+            lr_scheduler.step()
+            optimizer.zero_grad()
+
+if __name__ == '__main__':
+    train()
